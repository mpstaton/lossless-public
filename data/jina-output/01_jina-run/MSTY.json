{
  "code": 200,
  "status": 20000,
  "data": {
    "links": {
      "Msty": "https://msty.app/",
      "Pricing": "https://msty.app/pricing",
      "": "https://x.com/msty_app",
      "üéâ New in the latest release: Thinking/Reasoning Models, Deepseek R1, Llama Vision, Faster Rendering Engine, Model Compact Guage, Phi 4, and more!": "https://msty.app/changelog",
      "Docs": "https://docs.msty.app/",
      "Prompts Library": "https://msty.app/prompts-library",
      "Blog": "https://msty.app/blog",
      "YouTube": "https://youtube.com/@mstyapp",
      "License Overview": "https://msty.app/license",
      "Privacy": "https://msty.app/privacy",
      "Terms": "https://msty.app/tos"
    },
    "title": "Msty - Using AI Models made Simple and Easy",
    "description": "AI beyond just plain chat. Private, Offline, Split chats, Branching, Concurrent chats, Web Search, RAG, Prompts Library, Vapor Mode, and more. Perfect LM Studio, Jan AI, and Perplexity alternative. Use models from Open AI, Deepseek, Claude, Ollama, and HuggingFace in a unified interface.",
    "url": "https://msty.app/",
    "content": "Msty - Using AI Models made Simple and Easy\n===============                                          \n\nOpen main menu\n\n[Msty](https://msty.app/)\n\nCompare Us[Pricing](https://msty.app/pricing)Resources\n\n[](https://discord.gg/2QBw6XxkCC)[](https://x.com/msty_app)\n\n[üéâ New in the latest release: Thinking/Reasoning Models, Deepseek R1, Llama Vision, Faster Rendering Engine, Model Compact Guage, Phi 4, and more!](https://msty.app/changelog)\n\nThe easiest way to use local and online AI models\n=================================================\n\nWithout Msty: painful setup, endless configurations, confusing UI, Docker, command prompt, multiple subscriptions, multiple apps, chat paradigm copycats, no privacy, no control.\n\nWith Msty: one app, one-click setup, no Docker, no terminal, offline and private, unique and powerful features.\n\n![Image 1: Meta](https://msty.app/icons/meta.webp)![Image 2: OpenAI](https://msty.app/icons/openai.webp)![Image 3: Claude](https://msty.app/icons/claude.webp)![Image 4: Gemini](https://msty.app/icons/gemini.webp)![Image 5: Mistral](https://msty.app/icons/mistral.webp)![Image 6: LLaVa](https://msty.app/icons/llava.webp)![Image 7: Qwen](https://msty.app/icons/qwen.webp)![Image 8: Deepseek](https://msty.app/icons/deepseek.webp)\n\nUsing local AI models for free like the new Reasoning model from Deepseek is just a step away!\n\nDownload Msty\n\n![Image 9: Msty](https://msty.app/msty-screenshots/msty-hero-image-light.webp)\n\n![Image 10: Alexe Martel](https://msty.app/testimonials/alexe.webp)\"I just discovered Msty and I am in love. Completely. It‚Äôs simple and beautiful and there‚Äôs a dark mode  \nand you don‚Äôt need to be super technical to get it to work. It‚Äôs magical!\" - Alexe\n\nOffline-First, Online-Ready\n---------------------------\n\nMsty is designed to function seamlessly offline, ensuring reliability and privacy. For added flexibility, it also supports popular online model vendors, giving you the best of both worlds.\n\n* * *\n\nParallel Multiverse Chats\n-------------------------\n\nRevolutionize your research with split chats. Compare and contrast multiple AI models' responses in real-time, streamlining your workflow and uncovering new insights.\n\n![Image 11](https://msty.app/msty-screenshots/msty-feature-parallel-chatting-light.webp)\n\n* * *\n\nCraft your conversations\n------------------------\n\nMsty puts you in the driver's seat. Take your conversations wherever you want,  \nand stop whenever you're satisfied.\n\n  \n\n![Image 12: Matt Williams](https://msty.app/testimonials/mattwilliams.webp) \"...Its branching capabilities are more advanced than so many other tools. It's pretty awesome.\" - Matt Williams, Founding member of Ollama\n\nRegenerate model responses\n\nReplace an existing answer or create and iterate through several conversation branches.  \n  \nDelete branches that don't sound quite right.  \n  \nYou can also choose a different model for regeneration.\n\nDelve deeper\n\nClone your chats\n\n![Image 13: Powerful analytics for the modern marketer](https://msty.app/msty-screenshots/msty-feature-conversation-branches-light.webp)\n\n* * *\n\nSummon Real-Time Data\n---------------------\n\nStay ahead of the curve with our web search feature. Ask Msty to fetch live data into your conversations for unparalleled relevance and accuracy.\n\n![Image 14](https://msty.app/msty-screenshots/msty-feature-real-time-data-light.webp)\n\n* * *\n\nRAG done Right.\n---------------\n\nMsty's Knowledge Stack goes beyond a simple document collection.  \nLeverage multiple data sources to build a comprehensive information stack.\n\nImport files and folders\n\nCompose stacks using .txt, .md, .json, .jsonl, .csv, .epub, .docx files and more.\n\nConnect Obsidian vaults\n\nAdd YouTube transcriptions\n\nUse one or more Stacks\n\nKnowledge Stack Insights\n\n![Image 15: Powerful analytics for the modern marketer](https://msty.app/msty-screenshots/msty-feature-rag-light.webp)\n\n* * *\n\nUnified Access to Models\n------------------------\n\nUse any models from Hugging Face, Ollama and Open Router. Choose the best model  \nfor your needs and seamlessly integrate it into your conversations.\n\n![Image 16](https://msty.app/msty-screenshots/msty-feature-model-hub-light.webp)\n\n* * *\n\nPrompt Paradise\n---------------\n\nAccess a ready-made library of prompts to guide the AI model, refine responses,  \nand fulfill your needs. You can also add your own prompts to the library.\n\nAsk a question\n\nUse one of our 230+ curated prompts and ask away the topics you are interested in.  \n  \nFind example outputs for the prompt you are using in the prompts library.\n\nRefine a message\n\nInstruct your model\n\n![Image 17: Powerful analytics for the modern marketer](https://msty.app/msty-screenshots/msty-feature-quick-prompts-light.webp)\n\n* * *\n\nDiscover & Visualize\n--------------------\n\nGet ready to converse, delve, and flow! Our dynamic duo of features combines  \nthe power of delve mode's endless exploration with Flowchat‚Ñ¢'s intuitive visualization.\n\nGetting lost in the rabbit holes of knowledge discovery has never been more fun!\n\n![Image 18](https://msty.app/msty-screenshots/msty-feature-flowchat-light.webp)\n\n* * *\n\nAnd many more features...\n-------------------------\n\n##### Ollama Integration\n\nUse your existing models with Msty\n\n##### Ultimate Privacy\n\nNo personal information ever leaves your machine\n\n##### Offline Mode\n\nUse Msty when you're off the grid\n\n##### Folders\n\nOrganize your chats into folders\n\n##### Multiple Workspaces\n\nIsolate data into their own spaces and sync across devices\n\n##### Attachments\n\nAttach images and documents to your chat\n\nSee what people are saying\n--------------------------\n\nYes, these are real comments from real users. [Join our Discord](https://discord.gg/2QBw6XxkCC) to say hi to them.\n\nMSTY‚Äôs been a powerful tool for us to unlock employee efficiency while remaining model-agonistic. The combination of a great UI and powerful, accessible features like prompt libraries, knowledge files and local LLMs has been huge for our employees.\n\n* * *\n\n![Image 19: Msty](https://msty.app/testimonials/eytan.webp)\n\nEytan Buchman\n\nCMO | Freightos Group\n\nThis is fantastic! I love how you've lowered the barrier to entry for us to get into working with AI.\n\n* * *\n\n![Image 20: Msty](https://msty.app/testimonials/admiralnines.webp)\n\n@admiralnines\n\nThe best Interface I've found yet to work with local models, so thank you for releasing it!\n\n* * *\n\n![Image 21: Msty](https://msty.app/testimonials/francesco.webp)\n\nFrancesco (@francesco\\_36500)\n\n...arguably the most user-friendly local LLM experience available today. Msty fits the bill so well that we had to write a HOWTO post around how we got it installed on an M1 MacBook - and in that same post had to essentially disavow our previous suggestion to use ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n* * *\n\n![Image 22: Msty](https://msty.app/testimonials/pete.webp)\n\nPeter Thomas\n\nI just discovered Msty and I am in love. Completely. It‚Äôs simple and beautiful and there‚Äôs a dark mode and you don‚Äôt need to be super technical to get it to work. It‚Äôs magical!\n\n* * *\n\n![Image 23: Msty](https://msty.app/testimonials/alexe.webp)\n\nAlexe (@alexemartel)\n\nMsty has answered most of my demands for a simple local app for comparing frontier & local models and refining prompts with a great interface. Accessible to anyone who can learn to get an API key\n\n* * *\n\n![Image 24: Msty](https://msty.app/testimonials/techczech.webp)\n\nDominik Lukes (@techczech)\n\nThis is the client I would recommend to most users who don't want (or can't) mess around with the command prompt or Docker. So, it's already a big step forward. üëç\n\n* * *\n\n![Image 25: Msty](https://msty.app/testimonials/niikv.webp)\n\n@niikv\n\n... of all Ollama GUI's this is definitely the best so far. Its branching capabilities are more advanced than so many other tools. It's pretty awesome.\n\n* * *\n\n![Image 26: Msty](https://msty.app/testimonials/mattwilliams.webp)\n\nMatt Williams (@Technovangelist)  \nFounding member of the Ollama team\n\nOne of the simplest ways I've found to get started with running a local LLM on a laptop (Mac or Windows). I've been using this for the past several days, and am really impressed. Whether you're interested in starting in open source local models, concerned about your data and privacy, or looking for a simple way to experiment as a developer, Msty is a great place to start.\n\n* * *\n\n![Image 27: Msty](https://msty.app/testimonials/mason-james.webp)\n\nMason James (@masonjames)\n\nI have tried MANY LLM (some paid ones) UI and I wonder why no one managed to build such beautiful, simple, and efficient before you ... üôÇ keep the good work!\n\n* * *\n\n![Image 28: Msty](https://msty.app/testimonials/oliver.webp)\n\nOlivier H. (@notan\\_ai)\n\nFantastic app - and in such a short timeframe ‚ù§Ô∏è I have been using ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà up until now, and (now) I prefer Msty.\n\n* * *\n\n![Image 29: Msty](https://msty.app/testimonials/beerpowered.webp)\n\n@BeerPowered\n\nI really like the app so far, so thank you. I also have ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà installed for some other stuff, the terminal pops up with warnings, I start feeling stressed, but I like that your app is much more opaque. No terminal outputs on your screen. Very normie friendly\n\n* * *\n\n![Image 30: Msty](https://msty.app/testimonials/kayaorsan.webp)\n\nKaya Orsan (@kayaorsan)\n\nMust say again how fantastic and great this app is. I enjoy Msty more and more every day. I use the heck out of it. The little quality of life features are perfect, stuff i didnt even know I needed and now I can't imagine not having them.\n\n* * *\n\n![Image 31: Msty](https://msty.app/testimonials/beyond.webp)\n\nBeyond (@beyond1676)\n\nI self-host, so having a local assistant has been on my list of to-dos for a while. I've used a number of others in this space and they all required many dependencies or technical know-how. I was looking for something that my spouse could also download and easily use. MSTY checks all the boxes for us. Easy setup (now available in Linux flavor!), local storage (security/privacy amirite?), model variety (who doesn't like model variety?), simple clean interface. The only drawback would be your computer and the requirements needed to run some of the larger more powerful models. 5/7 would download again\n\n* * *\n\n![Image 32: Msty](https://msty.app/testimonials/steve.webp)\n\n@/steve\n\nChiming in to say it looks really good and professional! It‚Äôs definitely ‚Äúenterprise level‚Äù in a good way, I was initially searching for a ‚Äúpricing‚Äù tab lol.\n\n* * *\n\n![Image 33: Msty](https://msty.app/testimonials/user_7832.webp)\n\n@user\\_7832\n\nMy brother isn‚Äôt a coder at all and he‚Äôs gotten very into ChatGPT over the last year or so, but a few weeks ago he actually cancelled his subscription in favour of Msty. You‚Äôre making a good product dude üôè\n\n* * *\n\n![Image 34: Msty](https://msty.app/testimonials/7ormen7.webp)\n\n@7ormen7\n\njust came here to say i just discovered msty from the official ollama github and that ya'll are doing incredible work üôè\n\n* * *\n\n![Image 35: Msty](https://msty.app/testimonials/soyhenry.webp)\n\n@soyhenry\n\nQuestions you may have\n----------------------\n\nDo you collect any telemetry data?\n\nIs it possible to use Msty offline?\n\nWhat about privacy?\n\nCan I use my existing models from Ollama?\n\nWhy should I trust Msty?\n\nIs Msty free?\n\nIs Msty available on mobile?\n\nI've some feedback/questions. How can I get in touch?\n\nReady to get started?\n---------------------\n\nWe are available in Discord if you have any questions or need help.\n\nDownload Msty\n\n[Pricing](https://msty.app/pricing)[Docs](https://docs.msty.app/)[Changelog](https://msty.app/changelog)[Prompts Library](https://msty.app/prompts-library)[Blog](https://msty.app/blog)[YouTube](https://youtube.com/@mstyapp)[License Overview](https://msty.app/license)[Privacy](https://msty.app/privacy)[Terms](https://msty.app/tos)\n\n[](https://discord.gg/2QBw6XxkCC)[](https://x.com/msty_app)\n\nMade with sipping lots of ‚òïÔ∏è by the bank of the Scioto River in Columbus, Ohio. If the world runs out of coffee, blame our CloudStack, LLC Team.",
    "usage": {
      "tokens": 2961
    }
  }
}