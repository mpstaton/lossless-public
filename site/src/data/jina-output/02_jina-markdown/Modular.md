Modular: A Fast, Scalable Gen AI Inference Platform
===============
            

[](https://www.modular.com/)

[Product](https://www.modular.com/#)

[![Image 1](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67c78d38ee305321668a7905_nav_01.jpg) Install Max Easiest way to serve and deploy GenAI](https://docs.modular.com/max/get-started)

[MAX Platform Modular Accelerated Xecution](https://www.modular.com/max)[Licensing Free for most users, paid support for scaled enterprise deployments](https://www.modular.com/pricing)[Talk to an AI Expert See if MAX is right for your unique needs](https://www.modular.com/company/talk-to-us)

[Solutions](https://www.modular.com/#)

[![Image 2](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67c78d386e135ab0b88d9f41_nav_02.jpg) Agents AI systems for tasks and decision-making](https://www.modular.com/max/solutions/agent)

[RAG & CAG AI retrieval and controlled generation ![Image 3](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c79934ba816285c09c0aa8_rag-cag.svg)![Image 4](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c7ad2f4075557b2a1a8c16_rag-cag.svg)](https://www.modular.com/max/solutions/rag-cag)

[Chatbots Automate conversations and interactions ![Image 5](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c7993fb3b8fc88faec47a4_chatbot.svg)![Image 6](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c7ad481c753b369b91c930_chatbot.svg)](https://www.modular.com/max/solutions/chatbots)

[Code Generation Generate accurate and efficient code ![Image 7](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c799875073344154cb5a16_code-generation.svg)![Image 8](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c7ad5997367fe0fb468f02_code-generation.svg)](https://www.modular.com/max/solutions/code-generation)

[Batch processing Improve efficiency and resource utilization ![Image 9](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c7999234e144ab6df3055b_batch-processing.svg)![Image 10](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c7ad50261fa78c08b571fe_batch-processing.svg)](https://www.modular.com/max/solutions/batch-processing)

[AI Inference Fast, Scalable AI Inference ![Image 11](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c799aa4c8b394e6b7265b3_ai-inference.svg)![Image 12](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c7ad64261fa78c08b57e6c_ai-inference.svg)](https://www.modular.com/max/solutions/ai-inference)

[Research Advanced model & kernel development ![Image 13](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c799b85e8eb414778ba6a5_research.svg)![Image 14](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67c7ad6d1c753b369b91e16d_research.svg)](https://www.modular.com/max/solutions/research)

[Resources](https://www.modular.com/#)

[![Image 15](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67c78d381766513cb04f441c_nav_03.jpg) MAX Models The easiest way to start using MAX](https://builds.modular.com/)

[Docs Get up and running. Fast.](https://docs.modular.com/max)[Tutorials Build amazing things](https://docs.modular.com/max/tutorials)[Blog Get the latest updates](https://www.modular.com/blog)

Build

*    [![Image 16](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67c79e51dcc06e393a3bc214_MAX-Astronaut.svg)MAX GenAI models](https://builds.modular.com/)
*    [![Image 17](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67c79e51dcc06e393a3bc214_MAX-Astronaut.svg)MAX Quickstart](https://docs.modular.com/max/get-started/)
*    [![Image 18](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67c79e51dcc06e393a3bc214_MAX-Astronaut.svg)MAX Examples](https://builds.modular.com/?category=recipes)
*   [🔥 Mojo Playground](https://docs.modular.com/mojo/playground/)

Connect

*   [Community](https://www.modular.com/community)
*   [Community Forum](https://forum.modular.com/)
*   [Community Highlights](https://www.modular.com/modverse)
*   [MAX Changelog](https://docs.modular.com/max/changelog/)

[Company](https://www.modular.com/#)

[![Image 19](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67c7a8f37c4077bbc6ba66c1_nav_about.jpg) About Build AI for anyone, anywhere.](https://www.modular.com/company/about)

[Careers We’re currently hiring!](https://www.modular.com/company/careers)[Culture What we believe](https://www.modular.com/company/culture)[Contact Us Talk to an AI Expert](https://www.modular.com/company/talk-to-us)

[](https://github.com/modular)[](https://discord.gg/modular)[](https://x.com/modular)[](https://youtube.com/@modularinc)[](https://linkedin.com/company/modular-ai)

[Enterprise](https://www.modular.com/enterprise)[Docs](https://docs.modular.com/max/)

[Get Started](https://docs.modular.com/max/)[Talk to an AI Expert](https://www.modular.com/company/talk-to-us)

[Talk to Us](https://www.modular.com/company/talk-to-us)[Get started](https://docs.modular.com/max/get-started)

[close](https://www.modular.com/#)

Serve & deploy AI models with
=============================

Serve advanced LLMs effortlessly. Bundle once, deploy anywhere, all from a unified codebase.

[Browse ALL models](https://builds.modular.com/?category=featured)[What is MAX?](https://www.modular.com/max)

[DeepSeek-R1 ![Image 20](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39828545f6a947b7438aa_deepseek.svg) DeepSeek](https://builds.modular.com/models/DeepSeek-R1-Distill-Llama/8B-Q4_K_M)

[LLama3.2 vision ![Image 21](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.2-Vision-Instruct/11B)

[QWEN 2.5 ![Image 22](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a3998b74f773bdec6919e9_alibaba.svg) Alibaba](https://builds.modular.com/models/Qwen2.5-Instruct/0.5B)

[Mistral ![Image 23](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39955dce95ccfbd8e20db_mistal-ai.svg) Mistal Ai](https://builds.modular.com/models/Mistral-Small-Instruct-2501/24B)

[DeepSeek-R1 ![Image 24](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39828545f6a947b7438aa_deepseek.svg) DeepSeek](https://builds.modular.com/models/DeepSeek-R1-Distill-Llama/8B-Q4_K_M)

[LLama3.2 vision ![Image 25](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.2-Vision-Instruct/11B)

[QWEN 2.5 ![Image 26](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a3998b74f773bdec6919e9_alibaba.svg) Alibaba](https://builds.modular.com/models/Qwen2.5-Instruct/0.5B)

[Mistral ![Image 27](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39955dce95ccfbd8e20db_mistal-ai.svg) Mistal Ai](https://builds.modular.com/models/Mistral-Small-Instruct-2501/24B)

[DeepSeek-R1 ![Image 28](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39828545f6a947b7438aa_deepseek.svg) DeepSeek](https://builds.modular.com/models/DeepSeek-R1-Distill-Llama/8B-Q4_K_M)

[LLama3.2 vision ![Image 29](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.2-Vision-Instruct/11B)

[QWEN 2.5 ![Image 30](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a3998b74f773bdec6919e9_alibaba.svg) Alibaba](https://builds.modular.com/models/Qwen2.5-Instruct/0.5B)

[Mistral ![Image 31](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39955dce95ccfbd8e20db_mistal-ai.svg) Mistal Ai](https://builds.modular.com/models/Mistral-Small-Instruct-2501/24B)

[LLama3.2 ![Image 32](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.2-Instruct/1B-Q4_K_M)

[Phi4 ![Image 33](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a399ab4dfe61cd8d9a6ec6_microsoft.svg) Microsoft](https://builds.modular.com/models/phi-4/15B)

[LLama3.1 ![Image 34](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.1-Instruct/8B-Q4_K_M)

[Mistral - Nemo ![Image 35](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39955dce95ccfbd8e20db_mistal-ai.svg) Mistal Ai](https://builds.modular.com/models/Mistral-Nemo-Instruct-2407/15B)

[LLama3.2 ![Image 36](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.2-Instruct/1B-Q4_K_M)

[Phi4 ![Image 37](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a399ab4dfe61cd8d9a6ec6_microsoft.svg) Microsoft](https://builds.modular.com/models/phi-4/15B)

[LLama3.1 ![Image 38](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.1-Instruct/8B-Q4_K_M)

[Mistral - Nemo ![Image 39](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39955dce95ccfbd8e20db_mistal-ai.svg) Mistal Ai](https://builds.modular.com/models/Mistral-Nemo-Instruct-2407/15B)

[LLama3.2 ![Image 40](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.2-Instruct/1B-Q4_K_M)

[Phi4 ![Image 41](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a399ab4dfe61cd8d9a6ec6_microsoft.svg) Microsoft](https://builds.modular.com/models/phi-4/15B)

[LLama3.1 ![Image 42](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a398b6726e5097436e6db4_meta.svg) Meta](https://builds.modular.com/models/Llama-3.1-Instruct/8B-Q4_K_M)

[Mistral - Nemo ![Image 43](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/67a39955dce95ccfbd8e20db_mistal-ai.svg) Mistal Ai](https://builds.modular.com/models/Mistral-Nemo-Instruct-2407/15B)

NEW: MAX RECIPES

Deploy GenAI using MAX with step-by-step guides
-----------------------------------------------

[Continuous Chat App Build a functional chat application using Llama 3 and MAX Serve. Get the code](https://builds.modular.com/recipes/max-serve-continuous-chat)[Generate Embeddings Generate Embeddings with MAX Serve, using an OpenAI-compatible API. Get the code](https://builds.modular.com/recipes/max-serve-openai-embeddings)[Offline Inference Pair MAX with Hugging Face to perform inference locally and efficiently. Get the code](https://builds.modular.com/recipes/max-offline-inference)[OpenAI Function Calling Create AI Agents with MAX Serve and OpenAI Function Calling Get the code](https://builds.modular.com/recipes/max-serve-openai-function-calling)[AI Weather Agent Build an intelligent weather assistant with MAX Serve, FastAPI and NextJS. Get the code](https://builds.modular.com/recipes/ai-weather-agent)[Multi-Modal Structured Output With Llama 3.2 Vision, MAX Serve and Pydantic Get the code](https://builds.modular.com/recipes/max-serve-multimodal-structured-output)

SOTA performance for GenAI workloads on Open LLMs
-------------------------------------------------

3940

Output Throughput (tok/s)

MODEL: LLAMA-3.1-8B-INSTRUCT

GPU: NVIDIA A100 80GB

WORKLOAD: SHAREGPT (500 PROMPTS)

[Read how we measure performance at Modular. Read More ![Image 44](https://cdn.prod.website-files.com/63f9f100025c058594957cca/6761623ed89e339ef03bd6f8_performance-measure%20(1).jpg)](https://modular.com/blog/max-gpu-state-of-the-art-throughput-on-a-new-genai-platform)

18+

REQUEST THROUGHPUT (REQ/s)

95% +

GPU UTILIZATION

[Talk to an AI Expert](https://www.modular.com/company/talk-to-us)

Get started with MAX in under 3 minutes
---------------------------------------

Try these tutorials to see how fast, simple and customizable owning your AI infrastructure can be with MAX.

[![Image 45](https://cdn.prod.website-files.com/63f9f100025c058594957cca/677f2565d9941bbd0991d650_tut-icon-llama.svg) Deploy Llama 3 on GPU with MAX Serve Read Tutorial](https://docs.modular.com/max/tutorials/max-serve-local-to-cloud/)[![Image 46](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67801cec1471b0d235ab1910_tutorial-kubernetes.svg) Deploy Llama 3 on GPU-powered Kubernetes clusters Read Tutorial](https://docs.modular.com/max/tutorials/deploy-max-serve-on-kubernetes/)[![Image 47](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67801cfa1189daf67cd247b7_tutorial-python.svg) Get started with MAX Graph in Python Read Tutorial](https://docs.modular-staging.com/max/tutorials/get-started-with-max-graph-in-python/)[![Image 48](https://cdn.prod.website-files.com/63f9f100025c058594957cca/67801d0a8fc249a58d14290a_tutorial-hf.svg) Deploy a PyTorch model from Hugging Face Read Tutorial](https://docs.modular.com/max/tutorials/deploy-pytorch-llm/)

Iterate quickly from your laptop
--------------------------------

Develop, test, and deploy in a unified environment that eliminates inconsistencies and accelerates your time to production.

![Image 49](https://cdn.prod.website-files.com/63f9f100025c058594957cca/675c166b675b5156750a8e87_codebase-macbook%20(1).jpg)

![Image 50](https://cdn.prod.website-files.com/63f9f100025c058594957cca/6760ce0d3641e83ded77bc85_codebase_desktop.svg)![Image 51](https://cdn.prod.website-files.com/63f9f100025c058594957cca/6760ce0a9c1651741a9659ae_codebase_respo.svg)

OR

### Deploy to any cloud VPC or Kubernetes, using the same codebase

Deploy to any cloud provider with ease, ensuring flexibility and scalability without having to reconfigure for different environments.

A portable GPU software stack that gives you options
----------------------------------------------------

[Easy to switch Have a PoC that's working with closed proprietary model and now you're ready to own your AI stack? That's what MAX does best! Get started with MAX](https://docs.modular.com/max/)[Use any open source model Run Llama 3.1 now with MAX now, or use an open source model using the tutorial linked below. Read Tutorial](https://docs.modular.com/max/tutorials/deploy-pytorch-llm/)

Run on GPU or CPU

Get great performance and utilization across all your instances.

[Deploy to any cloud Deploy yourself anywhere, or book time with our support team to help you configure your business critical applications and connect to your inference pipelines. Talk to us](https://www.modular.com/company/contact)

Own, control, and secure your AI future
---------------------------------------

[Get off the endpoint Gain full control over performance, security, and optimization.](https://docs.modular.com/max/tutorials/deploy-pytorch-llm/)[Manage your data privacy & compliance Get peace of mind with MAX. Own your ML Pipelines and avoid sending your proprietary data to external sources.](https://www.modular.com/blog/take-control-of-your-ai)[Own your IP Control every layer of your stack. Get your weights from anywhere. Customize down to the kernel if needed.](https://www.modular.com/blog/take-control-of-your-ai)

“Once we got to proof of concept, we knew we needed to bring it in house. MAX was that solution.“

![Image 52](https://cdn.prod.website-files.com/63f9f100025c058594957cca/675f6801c33d823d170c24bf_mrwilliams12.jpg)Fortune 100 Customer

FAQ
---

How do I use MAX?

The Modular Accelerated Xecution (MAX) platform is a unified set of APIs and tools that simplify the process of building and deploying your own high-performance AI endpoint. To get started with MAX either locally, or via a Docker Container, just [**Install MAX**](https://docs.modular.com/max/install) or follow one our tutorials like [**Deploying Llama 3 on GPU with MAX Serve**](https://docs.modular.com/max/tutorials/max-serve-local-to-cloud).

What does MAX replace?

We created MAX to solve the fragmented and confusing array of AI tools that plague the industry today. Our unified toolkit is designed to help the world build high-performance AI pipelines and deploy them to any hardware removing the need for hardware vendor specific libraries. [**You can read more in our blog post.**](https://www.modular.com/blog/introducing-max-24-6-a-gpu-native-generative-ai-platform)

How much do I have to pay to use MAX?

MAX is a free and permissive AI inference framework that enables developers and enterprises to develop and deploy AI inference workloads on any hardware type, into any type of environment (including into production). We offer MAX Enterprise for organizations seeking enterprise support, and you can read more on our [**Pricing Page**](https://www.modular.com/pricing).

Is MAX compatible with my current stack?

Almost certainly. MAX is built without vendor-specific hardware libraries, enabling it to scale effortlessly across a wide range of CPUs and GPUs. We tightly integrate with AI ecosystem tools such as Python, PyTorch, and Hugging Face, and have fully extensible API surface. MAX Serve is available in a ready-to-deploy containers, and provides an OpenAI API endpoint API surface. We work and deploy easily with Docker and Kubernetes. [**Read more here**](https://docs.modular.com/max/intro).

What models does MAX currently support?

MAX supports model formats provided by Hugging Face, PyTorch, ONNX and MAX Graphs (our model format). We have a fully integrated LLM serving and execution stack that provides SOTA performance out-of-the-box. [**You can read more about the models here**](https://docs.modular.com/max/model-formats).

Free to start. Scale as you grow.
---------------------------------

MAX is FREE for anyone to self-manage.  Looking for enterprise solutions and dedicated support?  Book a demo or reach out to our sales team.

[Book a demo See what MAX can do for you. ![Image 53](https://cdn.prod.website-files.com/63f9f100025c058594957cca/675c43db2679e85345b7b4dd_book-a-demo_visual.jpg)![Image 54](https://cdn.prod.website-files.com/63f9f100025c058594957cca/675f6b372fdc773e44d23fa2_max-lightning-dark%20(1).jpg)](https://www.modular.com/company/talk-to-us)[Start using MAX View our documentation ![Image 55](https://cdn.prod.website-files.com/63f9f100025c058594957cca/675f687df508e0c28f1188e5_max-cosmo_1.jpg)](https://docs.modular.com/max/)[MAX License See what license is best for you ![Image 56](https://cdn.prod.website-files.com/63f9f100025c058594957cca/675f687d23114ab56f87250d_max-cosmo_2.jpg)](https://www.modular.com/pricing)[Contact Us Reach out to our sales team ![Image 57](https://cdn.prod.website-files.com/63f9f100025c058594957cca/675f687d183c17fac566b3ab_max-cosmo_3.jpg)](https://www.modular.com/company/contact)

Developer Approved 👍
---------------------

“Max installation on Mac M2 and running llama3 in (q6\_k and q4\_k) was a breeze! Thank you Modular team!”

![Image 58](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0597cf79a454025fc448_nl.jpg)

NL

Mojo is Python++. It will be, when complete, a strict superset of the Python language. But it also has additional functionality so we can write high performance code that takes advantage of modern accelerators.

![Image 59](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0613d4ce5e99ea98b1c5_jeremyphoward.jpg)

jeremyphoward

“Tired of the two language problem. I have one foot in the ML world and one foot in the geospatial world, and both struggle with the "two-language" problem. Having Mojo - as one language all the way through would be awesome.”

![Image 60](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d062e2064c3d2db7e4ba9_fnands.jpg)

fnands

“Mojo can replace the C programs too. It works across the stack. It’s not glue code. It’s the whole ecosystem.”

![Image 61](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06430d00613aed4fe7f4_scrumtuous.jpg)

scrumtuous

“Max installation on Mac M2 and running llama3 in (q6\_k and q4\_k) was a breeze! Thank you Modular team!”

![Image 62](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0597cf79a454025fc448_nl.jpg)

NL

Mojo is Python++. It will be, when complete, a strict superset of the Python language. But it also has additional functionality so we can write high performance code that takes advantage of modern accelerators.

![Image 63](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0613d4ce5e99ea98b1c5_jeremyphoward.jpg)

jeremyphoward

“Tired of the two language problem. I have one foot in the ML world and one foot in the geospatial world, and both struggle with the "two-language" problem. Having Mojo - as one language all the way through would be awesome.”

![Image 64](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d062e2064c3d2db7e4ba9_fnands.jpg)

fnands

“Mojo can replace the C programs too. It works across the stack. It’s not glue code. It’s the whole ecosystem.”

![Image 65](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06430d00613aed4fe7f4_scrumtuous.jpg)

scrumtuous

“Max installation on Mac M2 and running llama3 in (q6\_k and q4\_k) was a breeze! Thank you Modular team!”

![Image 66](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0597cf79a454025fc448_nl.jpg)

NL

Mojo is Python++. It will be, when complete, a strict superset of the Python language. But it also has additional functionality so we can write high performance code that takes advantage of modern accelerators.

![Image 67](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0613d4ce5e99ea98b1c5_jeremyphoward.jpg)

jeremyphoward

“Tired of the two language problem. I have one foot in the ML world and one foot in the geospatial world, and both struggle with the "two-language" problem. Having Mojo - as one language all the way through would be awesome.”

![Image 68](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d062e2064c3d2db7e4ba9_fnands.jpg)

fnands

“Mojo can replace the C programs too. It works across the stack. It’s not glue code. It’s the whole ecosystem.”

![Image 69](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06430d00613aed4fe7f4_scrumtuous.jpg)

scrumtuous

“Max installation on Mac M2 and running llama3 in (q6\_k and q4\_k) was a breeze! Thank you Modular team!”

![Image 70](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0597cf79a454025fc448_nl.jpg)

NL

Mojo is Python++. It will be, when complete, a strict superset of the Python language. But it also has additional functionality so we can write high performance code that takes advantage of modern accelerators.

![Image 71](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0613d4ce5e99ea98b1c5_jeremyphoward.jpg)

jeremyphoward

“Tired of the two language problem. I have one foot in the ML world and one foot in the geospatial world, and both struggle with the "two-language" problem. Having Mojo - as one language all the way through would be awesome.”

![Image 72](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d062e2064c3d2db7e4ba9_fnands.jpg)

fnands

“Mojo can replace the C programs too. It works across the stack. It’s not glue code. It’s the whole ecosystem.”

![Image 73](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06430d00613aed4fe7f4_scrumtuous.jpg)

scrumtuous

“What @modular is doing with Mojo and the MaxPlatform is a completely different ballgame.”

![Image 74](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06430d00613aed4fe7f4_scrumtuous.jpg)

scrumtuous

“I am focusing my time to help advance @Modular. I may be starting from scratch but I feel it’s what I need to do to contribute to #AI for the next generation.”

![Image 75](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0672215809c4a81b8e61_mytechnotalent.jpg)

mytechnotalent

“Mojo and the MAX Graph API are the surest bet for longterm multi-arch future-substrate NN compilation”

![Image 76](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0687633feda23295d074_pagilgukey.jpg)

pagilgukey

“A few weeks ago, I started learning Mojo 🔥 and MAX. Mojo has the potential to take over AI development. It's Python++. Simple to learn, and extremely fast.”

![Image 77](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06ddf0e5a4601b75846d_svpino.jpg)

svpino

“Mojo destroys Python in speed. 12x faster without even trying. The future is bright!”

![Image 78](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06ddf0e5a4601b75846d_svpino.jpg)

svpino

“It’s fast which is awesome. And it’s easy. It’s not CUDA programming...easy to optimize.”

![Image 79](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/669579d06e974299c702008c_dorjeduck.png)

dorjeduck

“I tried MAX builds last night, impressive indeed. I couldn't believe what I was seeing... performance is insane.”

![Image 80](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/66c4eba7a639c8a15494bbe5_profile01.png)

drdude81

“What @modular is doing with Mojo and the MaxPlatform is a completely different ballgame.”

![Image 81](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06430d00613aed4fe7f4_scrumtuous.jpg)

scrumtuous

“I am focusing my time to help advance @Modular. I may be starting from scratch but I feel it’s what I need to do to contribute to #AI for the next generation.”

![Image 82](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0672215809c4a81b8e61_mytechnotalent.jpg)

mytechnotalent

“Mojo and the MAX Graph API are the surest bet for longterm multi-arch future-substrate NN compilation”

![Image 83](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0687633feda23295d074_pagilgukey.jpg)

pagilgukey

“A few weeks ago, I started learning Mojo 🔥 and MAX. Mojo has the potential to take over AI development. It's Python++. Simple to learn, and extremely fast.”

![Image 84](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06ddf0e5a4601b75846d_svpino.jpg)

svpino

“Mojo destroys Python in speed. 12x faster without even trying. The future is bright!”

![Image 85](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06ddf0e5a4601b75846d_svpino.jpg)

svpino

“It’s fast which is awesome. And it’s easy. It’s not CUDA programming...easy to optimize.”

![Image 86](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/669579d06e974299c702008c_dorjeduck.png)

dorjeduck

“I tried MAX builds last night, impressive indeed. I couldn't believe what I was seeing... performance is insane.”

![Image 87](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/66c4eba7a639c8a15494bbe5_profile01.png)

drdude81

“What @modular is doing with Mojo and the MaxPlatform is a completely different ballgame.”

![Image 88](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06430d00613aed4fe7f4_scrumtuous.jpg)

scrumtuous

“I am focusing my time to help advance @Modular. I may be starting from scratch but I feel it’s what I need to do to contribute to #AI for the next generation.”

![Image 89](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0672215809c4a81b8e61_mytechnotalent.jpg)

mytechnotalent

“Mojo and the MAX Graph API are the surest bet for longterm multi-arch future-substrate NN compilation”

![Image 90](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0687633feda23295d074_pagilgukey.jpg)

pagilgukey

“A few weeks ago, I started learning Mojo 🔥 and MAX. Mojo has the potential to take over AI development. It's Python++. Simple to learn, and extremely fast.”

![Image 91](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06ddf0e5a4601b75846d_svpino.jpg)

svpino

“Mojo destroys Python in speed. 12x faster without even trying. The future is bright!”

![Image 92](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06ddf0e5a4601b75846d_svpino.jpg)

svpino

“It’s fast which is awesome. And it’s easy. It’s not CUDA programming...easy to optimize.”

![Image 93](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/669579d06e974299c702008c_dorjeduck.png)

dorjeduck

“I tried MAX builds last night, impressive indeed. I couldn't believe what I was seeing... performance is insane.”

![Image 94](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/66c4eba7a639c8a15494bbe5_profile01.png)

drdude81

“What @modular is doing with Mojo and the MaxPlatform is a completely different ballgame.”

![Image 95](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06430d00613aed4fe7f4_scrumtuous.jpg)

scrumtuous

“I am focusing my time to help advance @Modular. I may be starting from scratch but I feel it’s what I need to do to contribute to #AI for the next generation.”

![Image 96](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0672215809c4a81b8e61_mytechnotalent.jpg)

mytechnotalent

“Mojo and the MAX Graph API are the surest bet for longterm multi-arch future-substrate NN compilation”

![Image 97](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0687633feda23295d074_pagilgukey.jpg)

pagilgukey

“A few weeks ago, I started learning Mojo 🔥 and MAX. Mojo has the potential to take over AI development. It's Python++. Simple to learn, and extremely fast.”

![Image 98](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06ddf0e5a4601b75846d_svpino.jpg)

svpino

“Mojo destroys Python in speed. 12x faster without even trying. The future is bright!”

![Image 99](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d06ddf0e5a4601b75846d_svpino.jpg)

svpino

“It’s fast which is awesome. And it’s easy. It’s not CUDA programming...easy to optimize.”

![Image 100](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/669579d06e974299c702008c_dorjeduck.png)

dorjeduck

“I tried MAX builds last night, impressive indeed. I couldn't believe what I was seeing... performance is insane.”

![Image 101](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/66c4eba7a639c8a15494bbe5_profile01.png)

drdude81

"Mojo gives me the feeling of superpowers. I did not expect it to outperform a well-known solution like llama.cpp."

![Image 102](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6529952ae16404c230e5a4b0_aydyn-tairov.jpeg)

Aydyn

"C is known for being as fast as assembly, but when we implemented the same logic on Mojo and used some of the out-of-the-box features, it showed a huge increase in performance... It was amazing."

![Image 103](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6529952ae16404c230e5a4b0_aydyn-tairov.jpeg)

Aydyn

“I'm excited, you're excited, everyone is excited to see what's new in Mojo and MAX and the amazing achievements of the team at Modular.”

![Image 104](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d07ab4a1060f8ca488aa5_Eprahim.jpg)

Eprahim

“I'm very excited to see this coming together and what it represents, not just for MAX, but my hope for what it could also mean for the broader ecosystem that mojo could interact with.”

![Image 105](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d07ca82f4190d646f1684_strangemonad.jpg)

strangemonad

It worked like a charm, with impressive speed. Now my version is about twice as fast as Julia's (7 ms vs. 12 ms for a 10 million vector; 7 ms on the playground. I guess on my computer, it might be even faster). Amazing.

![Image 106](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0b6e8ed8be96d58c7b26_Adalseno.jpg)

Adalseno

“The more I benchmark, the more impressed I am with the MAX Engine.”

![Image 107](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/66c4eaf6eeb28c59d05a7e39_30216247.png)

justin\_76273

"Mojo gives me the feeling of superpowers. I did not expect it to outperform a well-known solution like llama.cpp."

![Image 108](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6529952ae16404c230e5a4b0_aydyn-tairov.jpeg)

Aydyn

"C is known for being as fast as assembly, but when we implemented the same logic on Mojo and used some of the out-of-the-box features, it showed a huge increase in performance... It was amazing."

![Image 109](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6529952ae16404c230e5a4b0_aydyn-tairov.jpeg)

Aydyn

“I'm excited, you're excited, everyone is excited to see what's new in Mojo and MAX and the amazing achievements of the team at Modular.”

![Image 110](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d07ab4a1060f8ca488aa5_Eprahim.jpg)

Eprahim

“I'm very excited to see this coming together and what it represents, not just for MAX, but my hope for what it could also mean for the broader ecosystem that mojo could interact with.”

![Image 111](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d07ca82f4190d646f1684_strangemonad.jpg)

strangemonad

It worked like a charm, with impressive speed. Now my version is about twice as fast as Julia's (7 ms vs. 12 ms for a 10 million vector; 7 ms on the playground. I guess on my computer, it might be even faster). Amazing.

![Image 112](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0b6e8ed8be96d58c7b26_Adalseno.jpg)

Adalseno

“The more I benchmark, the more impressed I am with the MAX Engine.”

![Image 113](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/66c4eaf6eeb28c59d05a7e39_30216247.png)

justin\_76273

"Mojo gives me the feeling of superpowers. I did not expect it to outperform a well-known solution like llama.cpp."

![Image 114](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6529952ae16404c230e5a4b0_aydyn-tairov.jpeg)

Aydyn

"C is known for being as fast as assembly, but when we implemented the same logic on Mojo and used some of the out-of-the-box features, it showed a huge increase in performance... It was amazing."

![Image 115](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6529952ae16404c230e5a4b0_aydyn-tairov.jpeg)

Aydyn

“I'm excited, you're excited, everyone is excited to see what's new in Mojo and MAX and the amazing achievements of the team at Modular.”

![Image 116](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d07ab4a1060f8ca488aa5_Eprahim.jpg)

Eprahim

“I'm very excited to see this coming together and what it represents, not just for MAX, but my hope for what it could also mean for the broader ecosystem that mojo could interact with.”

![Image 117](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d07ca82f4190d646f1684_strangemonad.jpg)

strangemonad

It worked like a charm, with impressive speed. Now my version is about twice as fast as Julia's (7 ms vs. 12 ms for a 10 million vector; 7 ms on the playground. I guess on my computer, it might be even faster). Amazing.

![Image 118](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0b6e8ed8be96d58c7b26_Adalseno.jpg)

Adalseno

“The more I benchmark, the more impressed I am with the MAX Engine.”

![Image 119](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/66c4eaf6eeb28c59d05a7e39_30216247.png)

justin\_76273

"Mojo gives me the feeling of superpowers. I did not expect it to outperform a well-known solution like llama.cpp."

![Image 120](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6529952ae16404c230e5a4b0_aydyn-tairov.jpeg)

Aydyn

"C is known for being as fast as assembly, but when we implemented the same logic on Mojo and used some of the out-of-the-box features, it showed a huge increase in performance... It was amazing."

![Image 121](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/6529952ae16404c230e5a4b0_aydyn-tairov.jpeg)

Aydyn

“I'm excited, you're excited, everyone is excited to see what's new in Mojo and MAX and the amazing achievements of the team at Modular.”

![Image 122](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d07ab4a1060f8ca488aa5_Eprahim.jpg)

Eprahim

“I'm very excited to see this coming together and what it represents, not just for MAX, but my hope for what it could also mean for the broader ecosystem that mojo could interact with.”

![Image 123](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d07ca82f4190d646f1684_strangemonad.jpg)

strangemonad

It worked like a charm, with impressive speed. Now my version is about twice as fast as Julia's (7 ms vs. 12 ms for a 10 million vector; 7 ms on the playground. I guess on my computer, it might be even faster). Amazing.

![Image 124](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/668d0b6e8ed8be96d58c7b26_Adalseno.jpg)

Adalseno

“The more I benchmark, the more impressed I am with the MAX Engine.”

![Image 125](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/66c4eaf6eeb28c59d05a7e39_30216247.png)

justin\_76273

Start building with MAX

[Download MAX](https://docs.modular.com/max/)

Easy ways to get started
------------------------

*   [Get started guide With just a few commands, you can install MAX as a conda package and deploy a GenAI model on a local endpoint. Read Guide](https://docs.modular.com/max/get-started/)
*   [400+ open source models Follow step by step recipes to build Agents, chatbots, and more with MAX. Browse Models](https://builds.modular.com/?category=featured)
*   [Browse Examples Follow step by step recipes to build Agents, chatbots, and more with MAX. Go to Forum](https://builds.modular.com/?category=recipes)

*   Product
    
    *   [MAX](https://www.modular.com/max)
    *   [Max Enterprise](https://www.modular.com/enterprise)
    *   [License](https://www.modular.com/pricing)[Install](https://docs.modular.com/max/get-started/)
*   QUICK START
    
    *   [Documentation](https://docs.modular.com/max/)
    *   [Tutorials](https://docs.modular.com/max/tutorials)
    *   [MAX GenAI models](https://builds.modular.com/)
    *   [Run Llama3](https://builds.modular.com/models/Llama-3.2-Instruct/1B-Q4_K_M)
    *   [Run Deepseek-R1](https://builds.modular.com/models/DeepSeek-R1-Distill-Llama/8B-Q4_K_M)
*   Solutions
    
    [Max for AI Agents](https://www.modular.com/max/solutions/agent)
    
    [Max for AI Inference](https://www.modular.com/max/solutions/ai-inference)
    
    [Max for Batch processing](https://www.modular.com/max/solutions/batch-processing)
    
    [Max for Chatbots](https://www.modular.com/max/solutions/chatbots)
    
    [Max for Code Generation](https://www.modular.com/max/solutions/code-generation)
    
    [Max for RAG & CAG](https://www.modular.com/max/solutions/rag-cag)
    
    [Max for Research](https://www.modular.com/max/solutions/research)
    
*   DEVELOPERS
    
    *   [MAX Builds](https://builds.modular.com/)
    *   [MAX Changelog](https://docs.modular.com/max/changelog/)
    *   [Mojo🔥](https://www.modular.com/mojo)
    *   [Mojo 🔥 Playground](https://docs.modular.com/mojo/playground)
*   Connect
    
    *   [Blog](https://www.modular.com/blog)
    *   [Community](https://www.modular.com/community)
    *   [Community Highlights](https://www.modular.com/modverse)
    *   [Report an issue](https://www.modular.com/company/report-issue)
    *   [Talk to sales](https://www.modular.com/company/contact)
*   Company
    
    *   [About Us](https://www.modular.com/company/about)
    *   [Culture](https://www.modular.com/company/culture)
    *   [Careers](https://www.modular.com/company/careers)
    *   [Contact Us](https://www.modular.com/company/contact)

*   [](https://github.com/modular)
*   [](https://discord.gg/modular)
*   [](https://x.com/modular)
*   [](https://youtube.com/@modularinc)
*   [](https://linkedin.com/company/modular-ai)

Copyright © 2025 Modular Inc

[Terms](https://www.modular.com/legal/terms), [Privacy](https://www.modular.com/legal/privacy) & [Acceptable Use](https://www.modular.com/legal/aup)

![Image 126](https://static.scarf.sh/a.png?x-pxid=76124b6b-61e2-40f9-bed5-ca72890c648a)

MAX on GPU waiting list

Be the first to get lightning fast inference speed on your GPUs. Be the envy of all your competitors and lower your compute spend.

First name\*

Last name\*

Email\*

Job title\*

Contact Origin Type

Links/Buttons:
- [](https://linkedin.com/company/modular-ai)
- [Product](https://www.modular.com/#)
- [Install MaxEasiest way to serve and deploy GenAI](https://docs.modular.com/max/get-started)
- [MAX PlatformModular Accelerated Xecution](https://www.modular.com/max)
- [LicensingFree for most users, paid support for scaled enterprise deployments](https://www.modular.com/pricing)
- [Talk to an AI ExpertSee if MAX is right for your unique needs](https://www.modular.com/company/talk-to-us)
- [AgentsAI systems for tasks and decision-making](https://www.modular.com/max/solutions/agent)
- [RAG & CAGAI retrieval and controlled generation](https://www.modular.com/max/solutions/rag-cag)
- [ChatbotsAutomate conversations and interactions](https://www.modular.com/max/solutions/chatbots)
- [Code GenerationGenerate accurate and efficient code](https://www.modular.com/max/solutions/code-generation)
- [Batch processingImprove efficiency and resource utilization](https://www.modular.com/max/solutions/batch-processing)
- [AI InferenceFast, Scalable AI Inference](https://www.modular.com/max/solutions/ai-inference)
- [ResearchAdvanced model & kernel development](https://www.modular.com/max/solutions/research)
- [MAX ModelsThe easiest way to start using MAX](https://builds.modular.com/)
- [DocsGet up and running. Fast.](https://docs.modular.com/max)
- [TutorialsBuild amazing things](https://docs.modular.com/max/tutorials)
- [BlogGet the latest updates](https://www.modular.com/blog)
- [MAX Quickstart](https://docs.modular.com/max/get-started/)
- [MAX Examples](https://builds.modular.com/?category=recipes)
- [🔥 Mojo Playground](https://docs.modular.com/mojo/playground/)
- [Community](https://www.modular.com/community)
- [Community Forum](https://forum.modular.com/)
- [Community Highlights](https://www.modular.com/modverse)
- [MAX Changelog](https://docs.modular.com/max/changelog/)
- [AboutBuild AI for anyone, anywhere.](https://www.modular.com/company/about)
- [CareersWe’re currently hiring!](https://www.modular.com/company/careers)
- [CultureWhat we believe](https://www.modular.com/company/culture)
- [Enterprise](https://www.modular.com/enterprise)
- [Docs](https://docs.modular.com/max/)
- [Browse ALL models](https://builds.modular.com/?category=featured)
- [DeepSeek-R1DeepSeek](https://builds.modular.com/models/DeepSeek-R1-Distill-Llama/8B-Q4_K_M)
- [LLama3.2 visionMeta](https://builds.modular.com/models/Llama-3.2-Vision-Instruct/11B)
- [QWEN 2.5Alibaba](https://builds.modular.com/models/Qwen2.5-Instruct/0.5B)
- [MistralMistal Ai](https://builds.modular.com/models/Mistral-Small-Instruct-2501/24B)
- [LLama3.2Meta](https://builds.modular.com/models/Llama-3.2-Instruct/1B-Q4_K_M)
- [Phi4Microsoft](https://builds.modular.com/models/phi-4/15B)
- [LLama3.1Meta](https://builds.modular.com/models/Llama-3.1-Instruct/8B-Q4_K_M)
- [Mistral - NemoMistal Ai](https://builds.modular.com/models/Mistral-Nemo-Instruct-2407/15B)
- [Continuous Chat AppBuild a functional chat application using Llama 3 and MAX Serve.Get the code](https://builds.modular.com/recipes/max-serve-continuous-chat)
- [Generate EmbeddingsGenerate Embeddings with MAX Serve, using an OpenAI-compatible API.Get the code](https://builds.modular.com/recipes/max-serve-openai-embeddings)
- [Offline InferencePair MAX with Hugging Face to perform inference locally and efficiently.Get the code](https://builds.modular.com/recipes/max-offline-inference)
- [OpenAI Function CallingCreate AI Agents with MAX Serve and OpenAI Function CallingGet the code](https://builds.modular.com/recipes/max-serve-openai-function-calling)
- [AI Weather AgentBuild an intelligent weather assistant with MAX Serve, FastAPI and NextJS.Get the code](https://builds.modular.com/recipes/ai-weather-agent)
- [Multi-Modal Structured OutputWith Llama 3.2 Vision, MAX Serve and PydanticGet the code](https://builds.modular.com/recipes/max-serve-multimodal-structured-output)
- [Read how we measure performance at Modular.Read More](https://modular.com/blog/max-gpu-state-of-the-art-throughput-on-a-new-genai-platform)
- [Deploy Llama 3 on GPU with MAX ServeRead Tutorial](https://docs.modular.com/max/tutorials/max-serve-local-to-cloud/)
- [Deploy Llama 3 on GPU-powered Kubernetes clustersRead Tutorial](https://docs.modular.com/max/tutorials/deploy-max-serve-on-kubernetes/)
- [Get started with MAX Graph in PythonRead Tutorial](https://docs.modular-staging.com/max/tutorials/get-started-with-max-graph-in-python/)
- [Deploy a PyTorch model from Hugging FaceRead Tutorial](https://docs.modular.com/max/tutorials/deploy-pytorch-llm/)
- [Deploy to any cloudDeploy yourself anywhere, or book time with our support team to help you configure your business critical applications and connect to your inference pipelines.Talk to us](https://www.modular.com/company/contact)
- [Manage your data privacy & complianceGet peace of mind with MAX. Own your ML Pipelines and avoid sending your proprietary data to external sources.](https://www.modular.com/blog/take-control-of-your-ai)
- [Install MAX](https://docs.modular.com/max/install)
- [Deploying Llama 3 on GPU with MAX Serve](https://docs.modular.com/max/tutorials/max-serve-local-to-cloud)
- [You can read more in our blog post.](https://www.modular.com/blog/introducing-max-24-6-a-gpu-native-generative-ai-platform)
- [Read more here](https://docs.modular.com/max/intro)
- [You can read more about the models here](https://docs.modular.com/max/model-formats)
- [Mojo🔥](https://www.modular.com/mojo)
- [Mojo 🔥 Playground](https://docs.modular.com/mojo/playground)
- [Report an issue](https://www.modular.com/company/report-issue)
- [Terms](https://www.modular.com/legal/terms)
- [Privacy](https://www.modular.com/legal/privacy)
- [Acceptable Use](https://www.modular.com/legal/aup)
