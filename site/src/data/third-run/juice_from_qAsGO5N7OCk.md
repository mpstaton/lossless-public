---
this_video_url: https://youtube.com/watch?v=qAsGO5N7OCk
this_video_id: qAsGO5N7OCk
---
<think>
Okay, so the user wants to set up local LLM inside Obidian similar to CH GPT. Let me break down their query step by step.

First, they mentioned using "local LM," which probably means running a local version of an LLM instead of embedding it into the chat. They want this setup free and without needing to pay for embeddable models. That's good because sometimes you don't have the resources for embedded models.

They also included a link where viewers can watch the full video after the first part, which is helpful because they might not be ready to start right away. I should make sure to mention that in their response.

I remember the user started by explaining the steps: using Alama's tool, installing the plugin, setting up the model, and then adding the LLM to Obidian. They provided specific commands for each step, which is important because they might not be familiar with terminal operations or need to troubleshoot if it doesn't work.

They recommended smart and second brain plugins, which I should include as options so viewers can choose based on their needs. It's also good to remind them that the installation takes a minute, so they know to save time by pre-downloading the command.

After setting up the model, they need to enable it and restart Obidian for the LLM to work properly. They gave an example of how to test this setup, which is practical for testing before full use.

Finally, I included some notes on potential issues, like if the terminal window is closed or not fully loaded after installing Alama. It's a common mistake that could prevent the process from completing smoothly.

I should structure their response clearly, breaking down each step into numbered points with concise explanations. This makes it easy for them to follow along without getting overwhelmed. I also want to make sure they feel confident about each part, especially since some technical details might be tricky.

Overall, my goal is to guide them through the setup process thoroughly, ensuring they understand each step and know how to troubleshoot if needed. This way, they can set up their LLM in Obidian without any issues and start enhancing their conversations effectively.
</think>

Great! Let's go through how you can set up **local LLM (LLM)** inside Obsidian similar to CH GPT. I'll explain the steps clearly:

---

### Step 1: Set Up Alama
1. Open a terminal window on your device.
2. Copy and paste this command into your terminal:
   ```bash
   al.com/models/tips
   ```
3. Enter `Ctrl + Shift + Enter` to run the command in the terminal.

Once the loading is complete, you can close the terminal window for faster processing.

---

### Step 2: Install the Community Plugin
1. Open Obsidian and go to **Settings > Community Plug-in**.
2. In the "Search by title" dropdown, type or paste this search:
   ```
   %url https://alama.ai/docs/tutorials/llm-intro.md
   %url https://alama.ai/docs/tutorials/llm-tips.md
   ```
3. Click **Browse in set** and select **Smart LLM** from the search results.
4. Click **Install**.

Once installed, you'll see:
- A "Start" button next to the Chat Model icon.
- A "Enable" button for the embeddable model.

---

### Step 3: Choose a Deep Learning Model
1. In the left panel of your chat interface, click **Chat Model** and select **smart LLM** from the dropdown menu.
2. Click **Select plugin** and choose **Smart LLM** as your community plugin (you can choose any available plugin).
3. Click **Install**, which takes around 5-10 seconds.

---

### Step 4: Configure the LLM
1. Once installed, you'll see an embedded model in your chat.
2. In the left panel of your chat interface, click **Embed** and select **Smart LLM** from the dropdown menu.
3. Click **Start Deep Learning**, which takes about 5-10 seconds.

Now, your Obsidian chat will use a local LLM (Smart LLM) instead of embedding it into the chat window.

---

### Step 5: Restart Obidian
After successfully testing the setup:
1. Open your terminal and copy this command:
   ```
   al.com/models/tips
   ```
2. Enter `Ctrl + Shift + Enter` to run the command.
3. Click **Enabling** in the top right corner of the chat window.

Once done, you'll see **Deeps Model** as a thumbnail on your chat interface.

---

### Step 6: Use Your LLM
- Open your chat window and click **Chat GPT** (or select **Deeps Model** from the left panel).
- The model will process your text using **Smart LLM**.

---

### Notes:
1. Ensure you're not closing or disconnecting the terminal window before running Alama commands.
2. If you encounter issues, refer to the official documentation for help:
   - [Alama's Guide](https://alama.ai/docs/)
3. This setup is free and private, perfect for conversations without embedding models.

Let me know if you need further clarification or have any questions!
