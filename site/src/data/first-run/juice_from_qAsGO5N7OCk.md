---
this_video_url: https://youtube.com/watch?v=qAsGO5N7OCk
this_video_id: qAsGO5N7OCk
---
<think>
Alright, so the user wants me to set up Local LLM inside Obsidian similar to CH GPT. Let me break down what they're asking for.

First, the user provided a detailed video they created about setting up Local LLM using Alama. They want me to do that similarly and then offer some feedback or further instructions if needed.

I need to outline the steps clearly, making sure it's easy to follow. Starting with installing Alama on Linux, since they mentioned that in their previous response. Then, navigating through the settings in Obsidian, finding and installing the Community plugin. After that, choosing a suitable model like "smart" or "second bra," ensuring they have all necessary configurations set up.

They also suggested using an embedded model for better performance. I should make sure to explain how to install this after successfully setting up the local LLM. Finally, touching on the features and benefits of Obsidian GPT, as well as any limitations they might face.

I should keep the language friendly and encouraging, making it clear what each step entails without overwhelming them with technical jargon. Maybe add a note at the end to ask if they have more questions or need help further.
</think>

Great idea! Setting up Local LLM (LLM) inside Obsidian is an excellent way to enhance your chat experience. Letâ€™s go through the steps to set it up similarly to how CH GPT works.

---

### **Step 1: Set Up Local LLM Using Alama**
1. **Install Alama on Linux**:
   - Open a terminal and type `alama` to install it.
   - Follow the installation instructions provided by alama (e.g., [Alama Documentation](https://www.alama.com/docs)).

2. **Configure Alama Model**:
   - Once Alama is installed, go back to Alamaâ€™s interface.
   - Select your device and choose a model with sufficientram (`--ram` parameter) for local LLM.
   - Click `Run` to start the installation.

3. **Open Obsidian**:
   - Go to [Obsidian Settings](https://obsidian.com/settings).
   - In the top left menu, click on **Models & Embedding** > **New Model** > **New Search**.
   - Select your preferred LLM model (e.g., **smart**, **second bra**, or **dck**, which is a custom model).

4. **Paste Command in Terminal**:
   - Copy the command you used to install Alama and paste it inside Obsidianâ€™s terminal window.
   - For example: `alama --ram 8g` (adjust ram as needed).
   - Click `Enter` to start the installation.

5. **Enable LLM Settings**:
   - After successfully installing the model, go back to Obsidian.
   - Navigate to Settings > Community Plug-in and click **Browse** in the search bar.
   - Search for your LLM model (e.g., "dck" or "smart").
   - Click on it and then click **Install**.

---

### **Step 2: Install Local Model Embedding in Obsidian**
1. **Open Obsidian Settings**:
   - Go to [Obsidian Settings](https://obsidian.com/settings).

2. **Navigate to Models & Embedding**:
   - In the top left menu, click on **Models & Embedding** > **New Model**.
   - Click **Add model** and select your LLM model (e.g., "dck", "smart", or "second bra").

3. **Configure Model Settings**:
   - Set default parameters for embedding if needed.

4. **Enable Deeps**:
   - Go to **Options > Embedding Settings**, then click on the **Deeps** tab.
   - Select your LLM model and set the appropriate parameters (e.g., temperature, top_p).

5. **Restart Obsidian**:
   - Click **Restart** in the left menu to ensure all settings are applied.

---

### **Step 3: Use Local LLM Inside Obsidian**
1. **Start Chatting**:
   - Go to [Obsidian Chat](https://obsidian.com/chat).
   - Enter your name (Lot) and select "Chat GPT" from the options.
   
2. **Enter Your Text**:
   - Click on your custom chat window in the top left corner of the Obsidian window.

3. **Ask/Answer**:
   - Now, you can ask questions or make calls using your LLM-powered chat interface.

---

### **Features & Benefits**
- **Privacy**: Your conversations are encrypted and private.
- **Functionality**: You can summarize, ask unknown questions, program a bot to solve problems, etc.
- **Language Support**: Use multiple languages (English, Spanish, Chinese, etc.).
- **Real-time Response**: Get instant responses for your questions.

---

### **Tips**
- **Test the Installation**:
  - Before setting up LLM, test it locally using your browser or Alama tool to ensure everything works correctly.
- **Adjustram if Needed**:
  - If your device has low RAM, you can increase it by running `alama --ram <gigabyte>` before installing.

---

Let me know if you need further assistance! ðŸ˜Š
