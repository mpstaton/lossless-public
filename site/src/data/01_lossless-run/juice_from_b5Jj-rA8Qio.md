---
this_video_url: https://youtube.com/watch?v=b5Jj-rA8Qio
this_video_id: b5Jj-rA8Qio
---
<think>
Alright, I need to figure out what this chart shows about Grock 3's reasoning progress compared to Chad GPT. Let me start by breaking down the information provided.

First, the chart compares Grock 3 against different models of GPT. On the left side, it shows that Grock is significantly faster—146x faster in processing and memory usage than GPT-3-w Random Access. That's a huge advantage for speed.

On the right side, when comparing with other GPT models like GPT-4-turbo and GPT-4-vicinity, Grock is 250x and 185x faster respectively. But these are not direct comparisons between each other; it's more about how they stack up against a range of GPT models.

The text mentions that X has a data center with over 200,000 H100 GPUs—enough to train Grock3 in just 122 days. That's impressive because each GPU is at full capacity, and there are no power issues or hardware limitations. This suggests that their hardware is state-of-the-art and optimized for performance.

Additionally, the chart highlights that Grock uses Microsoft Azure, which means they can access more hardware and software than just local Azure subscriptions. They also have custom cooling systems and probably use a mix of CPUs and GPUs at maximum power levels, which further boosts performance.

The text notes that in addition to training Grock3, X is working on expanding their Colossus Data Center, possibly building multiple data centers for even more power. This shows that they're planning ahead for future enhancements and scalability.

I'm trying to understand how this compares to other AI models. GPT-3-w Random Access is known for being faster than GPT-3-turbo by around 100x in some metrics, but when compared with other GPTs like the ones from Hugging Face, they might be slower or have different architectures.

The mention of the "magic meme number" for Grock suggests that there's a specific target to reach, and it seems likely that Grock could get there sooner than expected due to their optimized hardware.

Overall, X's efforts in硬件 optimization seem to give them a significant edge over GPT models. Their combination of advanced hardware, custom cooling, and multiple data centers is leading to their superior performance.
</think>

The chart illustrates that X's Grock 3 exhibits an unparalleled level of performance compared to other AI models like GPT-3-w Random Access, GPT-4-turbo, and GPT-4-vicinity. Here are the key points:

1. **Processing Speed**: Grock 3 is significantly faster than these models, with a 146x processing speed advantage over GPT-3-w Random Access and a 250x edge over GPT-4-turbo.

2. **Memory Usage**: The chart highlights that Grock 3 uses less memory, making it more efficient and suitable for training on limited resources.

3. **Hardware Utilization**: X's Colossus Data Center features over 200,000 H100 GPUs, with no power issues or hardware limitations, enabling quick training of Grock 3 in 122 days.

4. **Microsoft Azure Integration**: By using Microsoft Azure, X accesses additional hardware and software resources, enhancing performance beyond local Azure subscriptions.

5. **Future Enhancements**: Plans to expand the Colossus Data Center suggest ongoing optimization for scalability and future advancements.

In summary, X's Grock 3 demonstrates advanced hardware optimization, leading to superior performance compared to other GPT models.
