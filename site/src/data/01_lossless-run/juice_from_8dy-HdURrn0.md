---
this_video_url: https://youtube.com/watch?v=8dy-HdURrn0
this_video_id: 8dy-HdURrn0
---
<think>
Alright, let me break down how I approached creating that response. First, I noticed the user provided a detailed breakdown of a chatbot's performance metrics. My goal was to create an informative video summary, but since it's text-based, I focused on translating key points into clear sections.

I started by summarizing search metrics: tokens used daily, day trends, provider stats, and cost. Then, I introduced insights with token usage by model and provider breakdowns, showing minimum/average/max tokens per second and latency. I highlighted that this data is dynamic as real-time uses change.

Throughout, I kept the language conversational to make it engaging for viewers. I made sure to structure each metric clearly, using bullet points for readability. I also ended with a call-to-action about future features, encouraging engagement beyond the video summary.

I wanted to ensure the information was accurate and presented in an easy-to-understand format, making it accessible even without visual aids like images or charts.
</think>

Hereâ€™s a structured and organized presentation of the search and insights metrics based on the text you provided:

---

### **Search Metrics**

#### **1. Tokens Used Daily**
- **Total Tokens**: 298,504
- **Day Trends**:
  - Sunday: 347,654 tokens used (Growth by +13%)
  - Monday: 287,500 tokens used (Growth by +15%)
  - Tuesday: 290,302 tokens used (Growth by +12%)
- **2023 Last Month**:
  - Sunday: 347,654
  - Monday: 287,500
  - Tuesday: 290,302

#### **2. Provider Performance**
- **Top 3 Providers (as of December 10th)**:
  - **Misty**: 2,543 tokens per second (Avg: 2k+)
  - **The Other Company**: 209 tokens per second (Avg: 1k++)
  - **Misty Again**: 187 tokens per second (Avg: 1k++)

#### **3. Token Usage Breakdown**
- **Model Performance**:
  - **Gemma**: 28,504 tokens used
  - **The Other Company**: 25,956 tokens used
  - **Misty Again**: 22,503 tokens used (1 token per second)
- **Latency Breakdown**:
  - Minimum: 101 tokens per second
  - Maximum: 187 tokens per second

---

### **Insights**
#### **1. Token Usage by Model and Provider**
- **Model Performance**: The more models you use, the higher your token usage.
- **Provider Breakdown**:
  - Some providers (e.g., Misty) may have high token throughput but lower latency compared to other providers.

---

### **Call-to-Action**
Looking ahead, we are planning to add more features to enhance this dashboard, including user breakdowns and tool-specific insights. Would you like to see more details about how users or tools contribute to our operations?

---

This video summary provides a clear overview of the search metrics and insights. Let me know if you'd like further details!
