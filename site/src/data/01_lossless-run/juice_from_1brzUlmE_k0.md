---
this_video_url: https://youtube.com/watch?v=1brzUlmE_k0
this_video_id: 1brzUlmE_k0
---
<think>
Okay, the user is sharing their experience with using AI to build a React Native Expo-based mobile app. They went through several stages of creating the app: landing page, core functionality development, UI/UX design, testing, and troubleshooting issues.

In each stage, they used two prompts in their AI model. The initial stage was relatively simple, so the first prompt handled most of the logic. As they added more features, like managing daily tasks and adding new ones, they needed a second prompt to refine the code, especially for handling user interactions and task input.

One issue they ran into was when trying to mark tasks as completed. The AI model got confused by text inputs and started resetting them every time a character was entered. That's an example of how even complex prompts can fail, which is frustrating but possible if there are ambiguities in the data or requirements.

They also had trouble with task input formatting, especially for new users who might not know how to structure their inputs correctly. This showed that while the AI model could handle certain tasks, it needs more precise guidance when dealing with specific formats.

The user then tried refining the prompts by adding context and constraints, which helped improve both feature development and UI/UX testing. It's clear that iterative improvement is key in using AI for such projectsâ€”each time refining based on feedback and trying again until the app works as intended.

Overall, the user seems proactive about their experience, so I'm happy to discuss how they can better handle similar challenges or perhaps explore other tools if needed.
</think>

It sounds like you were engaging with AI models effectively by using two prompts per stage. Here's a summary of your approach:

1. **Landing Page Design**: You used one prompt to generate the landing page design, focusing on visual appeal and user experience.

2. **Feature Development**:
   - For managing daily tasks: The first prompt focused on core logic, such as task prioritization and deadlines.
   - For adding new tasks: The second prompt was used to refine how users input task details (e.g., title, description).

3. **UI/UX Testing**: You tested UI elements like form validation and task input formatting.

4. **Troubleshooting**:
   - When marking tasks as completed: Issues with text inputs resuming resets.
   - For task input formatting: The second prompt helped clarify the expected structure of user input.

5. **Iterative Improvement**: You used a refined prompt each time, based on user feedback and specific issues encountered.

6. **Learning Experience**: This exercise highlighted how AI models can sometimes fail due to ambiguities in data or requirements, but also demonstrated how iterative improvement is key to success.

Let me know if you'd like further details on any aspect of your project!
